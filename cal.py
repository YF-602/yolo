import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import seaborn as sns
from pathlib import Path

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
    # box: [x_center, y_center, width, height]
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2
    
    # è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
    box1_x1 = x1 - w1/2
    box1_y1 = y1 - h1/2
    box1_x2 = x1 + w1/2
    box1_y2 = y1 + h1/2
    
    box2_x1 = x2 - w2/2
    box2_y1 = y2 - h2/2
    box2_x2 = x2 + w2/2
    box2_y2 = y2 + h2/2
    
    # è®¡ç®—äº¤é›†
    inter_x1 = max(box1_x1, box2_x1)
    inter_y1 = max(box1_y1, box2_y1)
    inter_x2 = min(box1_x2, box2_x2)
    inter_y2 = min(box1_y2, box2_y2)
    
    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    
    # è®¡ç®—å¹¶é›†
    box1_area = w1 * h1
    box2_area = w2 * h2
    union_area = box1_area + box2_area - inter_area
    
    return inter_area / union_area if union_area > 0 else 0

def read_label_files(labels_dir):
    """è¯»å–æ ‡ç­¾æ–‡ä»¶"""
    labels = {}
    for label_file in Path(labels_dir).glob('*.txt'):
        with open(label_file, 'r') as f:
            lines = f.readlines()
        boxes = []
        for line in lines:
            parts = line.strip().split()
            if len(parts) == 5:
                cls_id = int(parts[0])
                coords = [float(x) for x in parts[1:]]
                boxes.append((cls_id, coords))
        labels[label_file.stem] = boxes
    return labels
# e.g.
# {
#     '000016': [(2, [0.598802, 0.539, 0.592814, 0.798])], 
#     '000021': [(1, [0.346726, 0.259, 0.33631, 0.222]), 
#                (1, [0.266369, 0.601, 0.425595, 0.454]), 
#                (1, [0.81994, 0.513, 0.342262, 0.882])], 
#     '000023': [(1, [0.471557, 0.455, 0.583832, 0.906]), 
#                (2, [0.387725, 0.737, 0.757485, 0.526]), 
#                (1, [0.892216, 0.222, 0.215569, 0.396]), 
#                (1, [0.080838, 0.105, 0.155689, 0.206]), 
#                (2, [0.892216, 0.71, 0.215569, 0.532])], 
# }

def calculate_ap(recalls, precisions):
    """
    è®¡ç®—AP (Average Precision)
    PRæ›²çº¿ä¸‹çš„é¢ç§¯, è¶Šæ¥è¿‘1è¶Šå¥½
    """
    # å°†recallä»0åˆ°1è¿›è¡Œæ’å€¼
    mrec = np.concatenate(([0.], recalls, [1.]))
    mpre = np.concatenate(([0.], precisions, [0.]))
    
    # ç¡®ä¿precisionæ˜¯å•è°ƒé€’å‡çš„
    for i in range(len(mpre) - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])
    
    # è®¡ç®—AP
    indices = np.where(mrec[1:] != mrec[:-1])[0]
    ap = np.sum((mrec[indices + 1] - mrec[indices]) * mpre[indices + 1])
    
    return ap

def calculate_ap_for_threshold(tp_fp_list, gt_count, iou_threshold):
    """è®¡ç®—ç‰¹å®šIoUé˜ˆå€¼ä¸‹çš„AP"""
    if gt_count == 0:
        return 0
    
    # è¿‡æ»¤å‡ºæ»¡è¶³IoUé˜ˆå€¼çš„æ£€æµ‹
    valid_detections = [(tp, iou) for tp, iou in tp_fp_list if iou >= iou_threshold]
    
    if not valid_detections:
        return 0
    
    # è®¡ç®—precision-recallæ›²çº¿
    tp_cumsum = np.cumsum([tp for tp, _ in valid_detections])
    fp_cumsum = np.cumsum([1 - tp for tp, _ in valid_detections])
    
    precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)
    recalls = tp_cumsum / (gt_count + 1e-6)
    
    return calculate_ap(recalls, precisions)

def generate_complete_confusion_matrix(gt_labels_dict, pred_labels_dict, threshold, class_names, output_dir):
    """ç”Ÿæˆå®Œæ•´çš„ç›®æ ‡æ£€æµ‹æ··æ·†çŸ©é˜µï¼ˆå…¼å®¹å­—å…¸æ ¼å¼ï¼‰"""
    
    # e.g.(gt_labels_dict and pred_labels_dict)
    # {
    #     '000016': [(2, [0.598802, 0.539, 0.592814, 0.798])], 
    #     '000021': [(1, [0.346726, 0.259, 0.33631, 0.222]), 
    #                (1, [0.266369, 0.601, 0.425595, 0.454]), 
    #                (1, [0.81994, 0.513, 0.342262, 0.882])], 
    #     '000023': [(1, [0.471557, 0.455, 0.583832, 0.906]), 
    #                (2, [0.387725, 0.737, 0.757485, 0.526]), 
    #                (1, [0.892216, 0.222, 0.215569, 0.396]), 
    #                (1, [0.080838, 0.105, 0.155689, 0.206]), 
    #                (2, [0.892216, 0.71, 0.215569, 0.532])], 
    # }
    
    # æ‰©å±•ç±»åˆ«åç§°ï¼ŒåŠ å…¥èƒŒæ™¯
    extended_class_names = ['background'] + class_names
    num_classes = len(extended_class_names)
    
    # åˆå§‹åŒ–æ··æ·†çŸ©é˜µ (åŒ…å«èƒŒæ™¯)
    cm = np.zeros((num_classes, num_classes), dtype=int)
    
    # ç¡®ä¿å¤„ç†ç›¸åŒçš„å›¾åƒ
    common_image_names = sorted(set(gt_labels_dict.keys()) | set(pred_labels_dict.keys()))
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for img_name in common_image_names:
        gt_boxes = gt_labels_dict.get(img_name, [])
        pred_boxes = pred_labels_dict.get(img_name, [])
        
        used_gt = set()
        used_pred = set()
        
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†åŒ¹é…çš„æ£€æµ‹ (TPå’Œåˆ†ç±»é”™è¯¯)
        for i, pred_box in enumerate(pred_boxes):
            pred_cls, pred_coords = pred_box  # è§£åŒ… (class_id, [cx, cy, w, h])
            pred_class = pred_cls + 1  # +1å› ä¸º0æ˜¯èƒŒæ™¯
            
            best_iou = 0
            best_gt_idx = -1
            
            for j, gt_box in enumerate(gt_boxes):
                if j in used_gt:
                    continue
                gt_cls, gt_coords = gt_box  # è§£åŒ… (class_id, [cx, cy, w, h])
                iou = calculate_iou(pred_coords, gt_coords)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = j
            
            if best_gt_idx != -1 and best_iou >= threshold:
                # æœ‰åŒ¹é…çš„çœŸå®æ¡†
                gt_cls, _ = gt_boxes[best_gt_idx]
                gt_class = gt_cls + 1
                cm[pred_class, gt_class] += 1  # [é¢„æµ‹, çœŸå®]
                used_gt.add(best_gt_idx)
                used_pred.add(i)
            else:
                # æ²¡æœ‰åŒ¹é…çš„çœŸå®æ¡† â†’ æŠŠèƒŒæ™¯é¢„æµ‹ä¸ºç‰©ä½“
                cm[pred_class, 0] += 1  # [é¢„æµ‹ç±»åˆ«, èƒŒæ™¯]
                used_pred.add(i)
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ¼æ£€ (çœŸå®æ¡†æ²¡æœ‰è¢«åŒ¹é…)
        for j, gt_box in enumerate(gt_boxes):
            if j not in used_gt:
                gt_cls, _ = gt_box
                gt_class = gt_cls + 1
                cm[0, gt_class] += 1  # [èƒŒæ™¯, çœŸå®ç±»åˆ«]
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾è¡¨
    generate_confusion_matrix_plots(cm, extended_class_names, output_dir)
    
    return cm

def generate_confusion_matrix_plots(cm, class_names, output_dir):
    """ç”Ÿæˆæ··æ·†çŸ©é˜µçš„å¯è§†åŒ–"""
    
    # éå½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    plt.title('Complete Confusion Matrix (Including Background)')
    plt.xlabel('True Label')
    plt.ylabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'confusion_matrix_complete.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µï¼ˆæŒ‰è¡Œå½’ä¸€åŒ–ï¼‰
    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-6)
    cm_normalized = np.nan_to_num(cm_normalized)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Percentage'})
    plt.title('Normalized Confusion Matrix (Row-wise)')
    plt.xlabel('True Label')
    plt.ylabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'confusion_matrix_normalized.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # ç”Ÿæˆç®€åŒ–çš„æ··æ·†çŸ©é˜µï¼ˆä¸å«èƒŒæ™¯ï¼Œä¾¿äºæŸ¥çœ‹ç±»åˆ«é—´æ··æ·†ï¼‰
    if len(cm) > 1:
        cm_simple = cm[1:, 1:]  # å»æ‰èƒŒæ™¯è¡Œå’Œåˆ—
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm_simple, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names[1:], yticklabels=class_names[1:])
        plt.title('Simplified Confusion Matrix (Objects Only)')
        plt.xlabel('True Label')
        plt.ylabel('Predicted Label')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'confusion_matrix_simple.png'), 
                    dpi=300, bbox_inches='tight')
        plt.close()

# TODO ç”Ÿæˆçš„æ›²çº¿å½¢æ€è²Œä¼¼æœ‰ç‚¹é—®é¢˜
def generate_pr_curves(class_tp_fp, class_gt_count, class_names, output_dir):
    """ç”ŸæˆPRæ›²çº¿ã€Pæ›²çº¿ã€Ræ›²çº¿"""
    
    # PRæ›²çº¿
    plt.figure(figsize=(10, 8))
    
    for cls_id in range(len(class_names)):
        tp_fp_list = class_tp_fp[cls_id]
        if not tp_fp_list:
            continue
            
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        tp_fp_list.sort(key=lambda x: x[1], reverse=True)
        
        tp_cumsum = np.cumsum([tp for tp, _ in tp_fp_list])
        fp_cumsum = np.cumsum([1 - tp for tp, _ in tp_fp_list])
        
        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)
        recalls = tp_cumsum / (class_gt_count[cls_id] + 1e-6)
        
        # ç¡®ä¿æ›²çº¿ä»(0,0)å¼€å§‹
        recalls = np.concatenate(([0], recalls, [1]))
        precisions = np.concatenate(([1], precisions, [0]))
        
        plt.plot(recalls, precisions, label=f'{class_names[cls_id]}', linewidth=2)
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('PR Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'PR_curve.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # Pæ›²çº¿å’ŒRæ›²çº¿
    thresholds = np.linspace(0, 1, 100)
    
    plt.figure(figsize=(12, 5))
    
    # Pæ›²çº¿
    plt.subplot(1, 2, 1)
    for cls_id in range(len(class_names)):
        tp_fp_list = class_tp_fp[cls_id]
        if not tp_fp_list:
            continue
            
        precisions = []
        for threshold in thresholds:
            valid_detections = [tp for tp, iou in tp_fp_list if iou >= threshold]
            if not valid_detections:
                precisions.append(0)
                continue
            precision = sum(valid_detections) / len(valid_detections)
            precisions.append(precision)
        
        plt.plot(thresholds, precisions, label=f'{class_names[cls_id]}', linewidth=2)
    
    plt.xlabel('Confidence Threshold')
    plt.ylabel('Precision')
    plt.title('Precision Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Ræ›²çº¿
    plt.subplot(1, 2, 2)
    for cls_id in range(len(class_names)):
        tp_fp_list = class_tp_fp[cls_id]
        if not tp_fp_list:
            continue
            
        recalls = []
        for threshold in thresholds:
            valid_tp = sum(tp for tp, iou in tp_fp_list if iou >= threshold)
            recall = valid_tp / (class_gt_count[cls_id] + 1e-6)
            recalls.append(recall)
        
        plt.plot(thresholds, recalls, label=f'{class_names[cls_id]}', linewidth=2)
    
    plt.xlabel('Confidence Threshold')
    plt.ylabel('Recall')
    plt.title('Recall Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'P_R_curves.png'), dpi=300, bbox_inches='tight')
    plt.close()

def generate_f1_curve(class_tp_fp, class_gt_count, class_names, output_dir):
    """ç”ŸæˆF1æ›²çº¿"""
    
    thresholds = np.linspace(0, 1, 100)
    
    plt.figure(figsize=(10, 6))
    
    for cls_id in range(len(class_names)):
        tp_fp_list = class_tp_fp[cls_id]
        if not tp_fp_list:
            continue
            
        f1_scores = []
        for threshold in thresholds:
            valid_detections = [(tp, iou) for tp, iou in tp_fp_list if iou >= threshold]
            if not valid_detections:
                f1_scores.append(0)
                continue
                
            tp_count = sum(tp for tp, _ in valid_detections)
            fp_count = len(valid_detections) - tp_count
            fn_count = class_gt_count[cls_id] - tp_count
            
            precision = tp_count / (tp_count + fp_count + 1e-6)
            recall = tp_count / (tp_count + fn_count + 1e-6)
            f1 = 2 * precision * recall / (precision + recall + 1e-6)
            f1_scores.append(f1)
        
        plt.plot(thresholds, f1_scores, label=f'{class_names[cls_id]}', linewidth=2)
    
    plt.xlabel('Confidence Threshold')
    plt.ylabel('F1 Score')
    plt.title('F1 Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'F1_curve.png'), dpi=300, bbox_inches='tight')
    plt.close()

def generate_val_batch_plots(gt_dir, pred_dir, output_dir):
    """ç”Ÿæˆval_batch0_labelså’Œval_batch0_predå›¾è¡¨"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ ç”ŸæˆéªŒè¯æ‰¹æ¬¡å¯¹æ¯”å›¾çš„ä»£ç 
    # ç”±äºéœ€è¦å›¾åƒæ•°æ®ï¼Œè¿™é‡Œæš‚æ—¶è·³è¿‡
    pass

def evaluate_detection(gt_dir, pred_dir, output_dir='./evaluation_results'):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    # e.g.(gt_labels and pred_labels)
    # {
    #     '000016': [(2, [0.598802, 0.539, 0.592814, 0.798])], 
    #     '000021': [(1, [0.346726, 0.259, 0.33631, 0.222]), 
    #                (1, [0.266369, 0.601, 0.425595, 0.454]), 
    #                (1, [0.81994, 0.513, 0.342262, 0.882])], 
    #     '000023': [(1, [0.471557, 0.455, 0.583832, 0.906]), 
    #                (2, [0.387725, 0.737, 0.757485, 0.526]), 
    #                (1, [0.892216, 0.222, 0.215569, 0.396]), 
    #                (1, [0.080838, 0.105, 0.155689, 0.206]), 
    #                (2, [0.892216, 0.71, 0.215569, 0.532])], 
    # }

    gt_labels = read_label_files(gt_dir)
    pred_labels = read_label_files(pred_dir)
    
    # ç±»åˆ«åç§°
    class_names = ['car', 'person', 'bicycle']
    num_classes = len(class_names)
    
    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    confusion_data = []
    
    # ä¸ºæ¯ä¸ªç±»åˆ«å­˜å‚¨TP/FPä¿¡æ¯
    class_tp_fp = {i: [] for i in range(num_classes)}
    class_gt_count = {i: 0 for i in range(num_classes)}
    
    # å¤„ç†æ¯ä¸ªå›¾åƒ
    for img_name in gt_labels:
        gt_boxes = gt_labels[img_name]
        pred_boxes = pred_labels.get(img_name, [])
        
        # ç»Ÿè®¡çœŸå®æ¡†æ•°é‡
        for cls_id, _ in gt_boxes:
            class_gt_count[cls_id] += 1
        
        # ä¸ºæ¯ä¸ªé¢„æµ‹æ¡†æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çœŸå®æ¡†
        used_gt = set()
        
        for pred_cls, pred_coords in pred_boxes:
            best_iou = 0
            best_gt_idx = -1
            
            for gt_idx, (_, gt_coords) in enumerate(gt_boxes):
                if gt_idx in used_gt:
                    continue
                
                iou = calculate_iou(pred_coords, gt_coords)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx
            
            # è®°å½•TP/FP
            if best_gt_idx != -1 and best_iou >= 0.5 and pred_cls == gt_boxes[best_gt_idx][0]:
                class_tp_fp[pred_cls].append((1, best_iou))  # TP
                used_gt.add(best_gt_idx)
                confusion_data.append((gt_boxes[best_gt_idx][0], pred_cls))
            else:
                class_tp_fp[pred_cls].append((0, best_iou))  # FP
                if best_gt_idx != -1:
                    confusion_data.append((gt_boxes[best_gt_idx][0], pred_cls))
    
    # è®¡ç®—mAP50å’ŒmAP50-95
    aps_50 = []
    aps_50_95 = []
    
    for cls_id in range(num_classes):
        # æŒ‰ç½®ä¿¡åº¦æ’åº (è¿™é‡Œä½¿ç”¨IoUä½œä¸ºç½®ä¿¡åº¦ä»£ç†)
        tp_fp_list = class_tp_fp[cls_id]
        if not tp_fp_list:
            aps_50.append(0)
            aps_50_95.append(0)
            continue
        
        # æŒ‰IoUé™åºæ’åº
        tp_fp_list.sort(key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—ä¸åŒIoUé˜ˆå€¼ä¸‹çš„AP
        thresholds_50 = [0.5]
        thresholds_50_95 = np.arange(0.5, 1.0, 0.05)
        
        ap_50 = calculate_ap_for_threshold(tp_fp_list, class_gt_count[cls_id], 0.5)
        ap_50_95 = np.mean([calculate_ap_for_threshold(tp_fp_list, class_gt_count[cls_id], t) 
                           for t in thresholds_50_95])
        
        aps_50.append(ap_50)
        aps_50_95.append(ap_50_95)
    
    mAP50 = np.mean(aps_50)
    mAP50_95 = np.mean(aps_50_95)
    
    # ç”Ÿæˆå›¾è¡¨
    generate_complete_confusion_matrix(
        gt_labels_dict=gt_labels,           # çœŸå®æ ‡ç­¾å­—å…¸
        pred_labels_dict=pred_labels,       # é¢„æµ‹æ ‡ç­¾å­—å…¸  
        threshold=0.5,                      # IoUé˜ˆå€¼
        class_names=class_names,            # ç±»åˆ«åç§°
        output_dir=output_dir               # è¾“å‡ºç›®å½•
    )
    generate_pr_curves(class_tp_fp, class_gt_count, class_names, output_dir)
    generate_f1_curve(class_tp_fp, class_gt_count, class_names, output_dir)
    
    return mAP50, mAP50_95

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    gt_dir = './datasets/labels/test'      # çœŸå®æ ‡ç­¾ç›®å½•
    pred_dir = './output/labels/test'      # é¢„æµ‹æ ‡ç­¾ç›®å½•
    output_dir = './evaluation_results'    # è¾“å‡ºç›®å½•
    
    # æ‰§è¡Œè¯„ä¼°
    mAP50, mAP50_95 = evaluate_detection(gt_dir, pred_dir, output_dir)
    
    print("=" * 50)
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"mAP50: {mAP50:.4f}")
    print(f"mAP50-95: {mAP50_95:.4f}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("=" * 50)
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
    result_files = os.listdir(output_dir)
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file in result_files:
        print(f"  - {file}")